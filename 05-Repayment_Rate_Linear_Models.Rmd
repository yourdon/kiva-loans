05 Kiva Loans: Repayment Rate: Linear Models
===================================================

We are now ready to build a model that can help us predict the repayment rate for a loan. Once an analyst knows the repayment rate, it's a short jump to predicting the repayment date and taking appropriate measures when a loan is being repaid beyond the confidence intervals of the predicted rate. In aggregate, repayment rates can be used to gauge how quickly funds in the lending pool will be replenished and to create forecasts around what funding will be available to borrowers in the future.

Let's load our data and get going.

```{r, warning=FALSE, message=FALSE}
library(jsonlite)
library(ggplot2)
library(psych)
library(RColorBrewer)
library(boot)
library(leaps)
library(glmnet)

loans.mb = jsonlite::fromJSON("loans_mb.json")
loans.mb = na.omit(loans.mb) # just to be safe
```

## Simple linear models

So far, the variable that has seemed most promising when it comes to predicting the repayment rate is the loan amount. Let's try regressing only on that variable, simply to set a baseline. We'll begin with the multiple-borrower class, which we know to be an easier case, and return to the single-borrower class later. (The `glm` functions will standardize our features for us, it should be noted.)

```{r, warning=FALSE, message=FALSE}
# create test and training sets
set.seed(1)
mb.train.index = sample(dim(loans.mb)[1], dim(loans.mb)[1]/2) 
loans.mb.train = loans.mb[mb.train.index, ]
loans.mb.test = loans.mb[-mb.train.index, ]

glm.loan_amount.fit = glm(r.rate ~ terms.loan_amount, data=loans.mb.train) 
summary(glm.loan_amount.fit)  
```

Our coefficient and intercept are both significant. For future comparison, we note that the AIC is 50,219. (The penalty term is quite low, since in this first model, d = 1: there is only one predictor.) Another baseline we want is the root mean squared error (root MSE). Let's calculate that now.

```{r, warning=FALSE, message=FALSE}
glm.loan_amount.preds = predict(glm.loan_amount.fit, loans.mb.test,
                                type="response")
(mean((glm.loan_amount.preds - loans.mb.test$r.rate)^2))^.5
```

So the root MSE is about 3.55 USD per day. Given that the median repayment rate in our test set is 4.79 USD per day, and that the mean is 8.15 USD per day, this is not very good.

Do the model plots reveal anything of note?

```{r, warning=FALSE, message=FALSE}
plot(glm.loan_amount.fit)
```

The funnel shape in the residuals plot and the sigmoid-like shape in our QQ plot mean that our residuals don't have constant variance across the predictor space, and so we are violating one of the core assumption of a linear regression model. We also see this in the positive slope of the fit line in the scale-location plot. On the plus side, no points lie outside Cook's distance in the leverage plot.

## Log-linear model

To remedy the heteroscedastic effects above, we can try a log transformation of our repayment rate: 

```{r, warning=FALSE, message=FALSE}
glm.loan_amount.log.fit = glm(log(r.rate) ~ terms.loan_amount, data=loans.mb.train) 
summary(glm.loan_amount.log.fit) 
```

Both coefficients in the model are significant, and the AIC is much better: 12,360. However, the AIC here is the lucky beneficiary of a log transformation and thus can't be compared to the AIC above. A better comparison is between the root MSEs.

```{r, warning=FALSE, message=FALSE}
glm.loan_amount.log.preds = predict(glm.loan_amount.log.fit, loans.mb.test,
                                type="response")
(mean((exp(glm.loan_amount.log.preds) - loans.mb.test$r.rate)^2))^.5
```

Our root MSE (when transformed back to a rate) is 5.50 USD per day. This is markedly worse! Do we see a difference in the summary plots?

```{r, warning=FALSE, message=FALSE}
plot(glm.loan_amount.log.fit)
```

The QQ plot looks healthier, but the other plots, particularly the residuals plot, have picked up some new structure. Clearly, a log-transformation is not the way to go. Before we try anything more complex than a simple linear model, let's nail down a reasonable measure of our error using cross-validation. 

```{r, warning=FALSE, message=FALSE}
# cross-validation
set.seed(1)
glm.loan_amount.cv = cv.glm(loans.mb.test, glm.loan_amount.fit, K=10)
(glm.loan_amount.cv$delta[2])^.5
```

The cross-validation error for repayment rate is 3.48 USD per day, a little lower than our original estimate.

## Polynomial regression

One idea for remedying the structure in the residuals plot is to try a polynomial fit on our loan amount variable.

```{r, warning=FALSE, message=FALSE}
glm.loan_amount.poly.fit = glm(r.rate ~ poly(terms.loan_amount, 3, raw=T), 
                                   data=loans.mb.train) 
summary(glm.loan_amount.poly.fit) 
```

Given that added flexibility we have given the model, it's not surprising that the AIC is better than it was for our simple linear regression. How about the root MSE?

```{r, warning=FALSE, message=FALSE}
glm.loan_amount.poly.preds = predict(glm.loan_amount.poly.fit, loans.mb.test,
                                type="response")
(mean((glm.loan_amount.poly.preds - loans.mb.test$r.rate)^2))^.5
```

Our root MSE is 3.49 USD per day, which is comparable to the cross-validation error for the linear model. Is this borne out when we cross-validate on the polynomial fit? 

```{r, warning=FALSE, message=FALSE}
# cross-validation
set.seed(1)
glm.loan_amount.poly.cv = cv.glm(loans.mb.test, glm.loan_amount.poly.fit, K=10)
(glm.loan_amount.poly.cv$delta[2])^.5
```

The error drops to 3.42 USD per day -- a slight improvement -- although the residuals plot and QQ plot are still not what we would hope.

```{r, warning=FALSE, message=FALSE}
plot(glm.loan_amount.poly.fit)
```

## Multiple regression

Given the relatively high error of these single-variable regression models, we now wish to consider additional features. In order of increasing complexity, these features are:

* b.male (dichotomous)
* b.num (continuous)
* sector (10 levels)
* country (~50 levels)

Let's feed these to a forward-selection algorithm in the `leaps` package. First, we'll remove any countries that don't appear at least 30 times in our data set. 

```{r, error=FALSE, warning=FALSE}
high_countries = table(loans.mb$location.country_code) >= 30
hc_vec = high_countries[loans.mb$location.country_code] == TRUE
loans.mb.hc = loans.mb[hc_vec, ] 
```

We will allow up to 100 variables -- the high number is due to the fact that each country (minus 1) and each sector (minus 1) will become an indicator variable. We'll then see how each of the best p-dimensional models fares against all the best models with a different number of features.

```{r, error=FALSE, warning=FALSE}
fwd.fit = regsubsets(r.rate ~ terms.loan_amount + b.male + b.num +
                       location.country_code + sector, 
                     data=loans.mb.train, nvmax=100, method="forward")
#summary(fwd.fit)
```

We omit the painfully long summary read-out. The summary of the summary is: (1) loan amount is almost always the most important feature; (2) the male indicator variables sneaks in as a feature for 13-feature models, while the borrower number variables doesn't arrive until there are 49-feature models; (3) the most important sector feature is Retail, which shows up starting with 19-feature models.

The object that `regsubsets` returns allows us to look at the BIC value (a close cousin of the AIC) for each best-in-class model. The BIC is essentially a way of penalizing models as they add more predictors. Other means 

```{r, error=FALSE, warning=FALSE}
fit.bic = data.frame(x=1:length(summary(fwd.fit)$bic), y=summary(fwd.fit)$bic)

ggplot(data=fit.bic, aes(x=x, y=y)) + geom_line() +
  xlab("Number of variables") + ylab("BIC")
which.min(summary(fwd.fit)$bic)
```

The model with 27 variables has the lowest BIC, though the curve has essentially bottomed out by d = 22 variables. To slightly reduce complexity, we'll take the 22-variable model. What model did we just buy ourselves?

```{r, error=FALSE, warning=FALSE}
coef(fwd.fit, 22)
```

In addition to the loan amount and the male indicator variable, we have the Retail sector variable and about 20 country variables. What error does this model produce?

```{r, error=FALSE, warning=FALSE}
fwd.test = model.matrix(r.rate ~ ., data=loans.mb.test)
coef.22 = coef(fwd.fit, id=22)
fwd.pred = fwd.test[, names(coef.22)] %*% coef.22

(mean((loans.mb.test$r.rate - fwd.pred)^2))^.5
```

We have a root MSE of 2.94 USD per day, a definite improvement over the 3.42 USD per day that our polynomial regression on the loan amount got us. Although this improvement comes at the cost of model complexity, many of these additional "features" can be interpreted just as a single country feature (since each country gets dichotomized). The more serious problem is the size of the error.

## Interaction terms

We have not yet tried adding interaction terms to our multiple regression model. This is another avenue to explore. Let's sidestep our multi-level factors and add interaction terms for our loan amount variable and our male indicator variable, the latter of which was included in our multiple regression model above. Recall that we saw an interesting relationship between the male indicator variable and the repayment rate, where non-male groups tended to take out smaller loans and therefore tended to be associated with lower repayment rates, even though for high loan amounts, non-male groups repaid at a higher rate. Does this relationship express itself in an interaction term?

```{r, warning=FALSE, message=FALSE}
glm.loan_amount.interact.fit = glm(r.rate ~ terms.loan_amount*b.male, 
                              data=loans.mb.train) 
summary(glm.loan_amount.interact.fit)
```

The coefficient for `terms.loan_amoutn:b.maleTRUE` is significant and negative. This means that as the loan amount increases, the interaction term will increasingly pull down the rate when the indicator variable is 1 (i.e. when there is a male in the group). Around 2072 USD, the interaction term will eclipse the coefficient for the indicator term, and female groups will have a lower repayment rate.

This is interesting, but how is our error affected?

```{r, warning=FALSE, message=FALSE}
glm.loan_amount.interact.preds = predict(glm.loan_amount.interact.fit, loans.mb.test,
                                type="response")
(mean((glm.loan_amount.interact.preds - loans.mb.test$r.rate)^2))^.5
```

The error is 3.54 USD per day -- the same ballpark, in other words. Even if the interaction is significant, it doesn't help us.

## Continent over country?

Is it possible that using continent indicator variables -- rather than indicators for each country -- will help? We'll step quickly through the same routine:

```{r, warning=FALSE, message=FALSE}
glm.loan_amount.cont.fit = glm(r.rate ~ terms.loan_amount + continent, 
                              data=loans.mb.train) 
glm.loan_amount.cont.preds = predict(glm.loan_amount.cont.fit, loans.mb.test,
                                type="response")
(mean((glm.loan_amount.cont.preds - loans.mb.test$r.rate)^2))^.5
```

This is an improvement over the simple linear model, but the 22-feature model -- which did, in fact, comb through both countries and continents -- does better.

## Summary

What have we learned?

* For the multiple borrower class, a simple linear regression on the loan amount yields a root MSE of 3.48 USD per day. 
* Adding higher-order terms slightly improved our error, but not much. An interaction term between loan amount and the male indicator variable did not help.
* A 22-feature model that used some 20 countries brought the root MSE down to 2.94 USD per day.
* In all of the residuals plot we saw, there was some unwanted structure -- most notably, the funnel shape in the main simple regression model. The fact that the variance in the error increases for higher loan amounts is, in itself, not a bad thing. Because we are modeling a rate, this problem comes out in the wash. However, the presence of this structure indicates that we are abusing the assumptions made by linear models and that we should try different methods. 

Next, we look at tree-based methods for the multiple-borrower class, in order to see if we can find a more suitable model and bring our error down in the process.